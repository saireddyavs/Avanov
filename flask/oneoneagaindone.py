# -*- coding: utf-8 -*-
"""oneoneagaindone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aUDaUZWG1ZVHCLEDYtQCHtaDvGArLx31
"""

from twitter_scrapper import scraping
def nauman():
    import pandas as pd
    scraping()
    
    data = pd.read_csv('dezzex1.csv')
    
    

    #data.head()

    #post=[]
    #posl=[]
    negt=[]
    #negl=[]
    for i in data.index:
      if data['label'][i]<3:
        negt.append(data['tweet'][i])
        #negl.append(data['label'][i])
      #else:
        #post.append(data['Comment'][i])
        #posl.append(data['label'][i])
    negt = pd.DataFrame(negt)
    #negl = pd.DataFrame(negl)
    #post = pd.DataFrame(post)
    #posl = pd.DataFrame(posl)
    #negative = pd.concat([negt,negl],axis=1)
    negt.columns = ['tweet']
    #positive = pd.concat([post,posl],axis=1)
    #positive.columns = ['comments','label']

    #negative.to_csv("youtubenegative.csv")
    #positive.to_csv("youtubepositive.csv")

    #data = pd.read_csv('youtubenegative.csv')

    #data.tail()
    from sklearn.feature_extraction import text
    from sklearn.feature_extraction.text import TfidfVectorizer

    my_stop_words = text.ENGLISH_STOP_WORDS.union(["etihadairways","flight","email","airways","etihad","send"])



    from sklearn.feature_extraction.text import CountVectorizer
    cv = CountVectorizer(max_df=0.7,min_df=5,stop_words=my_stop_words)

    dtm = cv.fit_transform(negt['tweet'])

    from sklearn.decomposition import LatentDirichletAllocation

    LDA = LatentDirichletAllocation(n_components=4,random_state=42)

    LDA.fit(dtm)



    single_topic = LDA.components_[0]
    top_ten_words = single_topic.argsort()[-10:]

    #for index in top_ten_words:
      #print(cv.get_feature_names()[index])

    x=[]
    for i,topic in enumerate(LDA.components_):
      #print('TOPIC' ,i)
      x.append([cv.get_feature_names()[index] for index in topic.argsort()[-15:]])
      #print('\n')

    #nonnegative
    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf = TfidfVectorizer(max_df=0.75,min_df=3,stop_words=my_stop_words)

    dt = tfidf.fit_transform(negt['tweet'])

    from sklearn.decomposition import NMF
    nmf_model = NMF(n_components=4,random_state=42)
    nmf_model.fit(dt)

    y=[]
    for index,topic in enumerate(nmf_model.components_):
      #print('topic',index)
      y.append(([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]]))

    # Commented out IPython magic to ensure Python compatibility.

    # Load the library with the CountVectorizer method
    from sklearn.feature_extraction.text import CountVectorizer
    import numpy as np
    #import matplotlib.pyplot as plt
    #import seaborn as sns
    #sns.set_style('whitegrid')
    # %matplotlib inline
    # Helper function
    l=[]
    import numpy as np
    def plot_10_most_common_words(count_data, count_vectorizer):
        #import matplotlib.pyplot as plt
        words = count_vectorizer.get_feature_names()

        total_counts = np.zeros(len(words))
        for t in count_data:
            total_counts+=t.toarray()[0]

        count_dict = (zip(words, total_counts))
        count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:20]
        words = [w[0] for w in count_dict]
        counts = [w[1] for w in count_dict]
        x_pos = np.arange(len(words))

        #plt.figure(2, figsize=(15, 15/1.6180))
        #plt.subplot(title='10 most common words')
        #sns.set_context("notebook", font_scale=1.25, rc={"lines.linewidth": 2.5})
        #sns.barplot(x_pos, counts, palette='husl')
        #plt.xticks(x_pos, words, rotation=90)
        #plt.xlabel('words')
        l.append(words)
        #plt.ylabel('counts')
        #plt.show()
    # Initialise the count vectorizer with the English stop words
    count_vectorizer = CountVectorizer(stop_words='english')
    # Fit and transform the processed titles
    count_data = count_vectorizer.fit_transform(data['tweet'])
    # Visualise the 10 most common words
    plot_10_most_common_words(count_data, count_vectorizer)
    #print(l)

    print("WORKING ON IT.....")
    import nltk
    nltk.download('averaged_perceptron_tagger')
    selective_pos = ['NN','NNS','NNP','NNPS']
    z = []
    for i in l:
      for word,tag in nltk.pos_tag(i):
          if tag in selective_pos:
              z.append((word))
    return [x,y,z]


