{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import he_normal\n",
    "from keras.layers import BatchNormalization, Dense, Dropout, Flatten, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import L1L2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.Defaults.stop_words.add(\"virginamerica\")\n",
    "nlp.Defaults.stop_words.add(\"united\")\n",
    "nlp.Defaults.stop_words.add(\"unite\")\n",
    "nlp.Defaults.stop_words.add(\"delta\")\n",
    "nlp.Defaults.stop_words.add(\"southwest\")\n",
    "nlp.Defaults.stop_words.add(\"american\")\n",
    "nlp.Defaults.stop_words.add(\"us airways\")\n",
    "nlp.Defaults.stop_words.add(\"indigoairline\")\n",
    "nlp.Defaults.stop_words.add(\"indigo\")\n",
    "nlp.Defaults.stop_words.add(\"flight\")\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'\n",
    "def preprocess(tweet):\n",
    "    rev=re.sub('[^a-zA-Z]',' ',tweet)\n",
    "   \n",
    "    rev=rev.lower();\n",
    "    doc = nlp(rev, disable=['parser', 'ner'])\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "    tokens = [tok for tok in tokens if tok not in nlp.Defaults.stop_words and tok not in punctuations]\n",
    "    tokens = ' '.join(tokens)\n",
    "    twe=emoji_pattern.sub(r'', tokens)\n",
    "   \n",
    "    \n",
    "    return twe\n",
    "   \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def training(x,y):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=35)\n",
    "    pl = Pipeline([('tfidf',TfidfVectorizer()),('clf',LinearSVC())])\n",
    "    pl.fit(x_train,y_train)\n",
    "    predicts = pl.predict(x_test)\n",
    "    print(confusion_matrix(y_test,predicts))\n",
    "    print(classification_report(y_test,predicts))\n",
    "    print(\"accuracy::\",accuracy_score(y_test,predicts))\n",
    "    with open('model_svc.pkl','wb') as f:\n",
    "        pickle.dump(pl,f)\n",
    "    return pl,accuracy_score(y_test,predicts)\n",
    "\n",
    "def data_gen(data):\n",
    "    y=data['airline_sentiment']\n",
    "    le = LabelEncoder()\n",
    "    y=le.fit_transform(y)\n",
    "    pickle.dump(le,open('label_svm.pkl', 'wb'))\n",
    "    data.text = data.text.apply(lambda x: preprocess(x))\n",
    "    print(\"number of classes::\",len(list(le.classes_)))\n",
    "    nclasses=len(list(le.classes_))\n",
    "    x=data.text\n",
    "    return x,y,nclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    x=preprocess(x)\n",
    "    with open('model_svc.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "        return model.predict([x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data = pd.read_csv(\"Tweets.csv\")\n",
    "    print(\"done\")\n",
    "    x,y,nclasses=data_gen(data)\n",
    "    print(\"done\")\n",
    "#     svm_model,accuracy_svm=training(x,y)\n",
    "    print(\"done\")\n",
    "    lstm_model,accuracy_lstm=lstm_training(x,data['airline_sentiment'])\n",
    "    \n",
    "    if(accuracy_lstm>=accuracy_svm):\n",
    "        with open('best_model/model.pkl','wb') as f:\n",
    "            pickle.dump(lstm_model,f)\n",
    "            print(\"LSTM\")\n",
    "    else:\n",
    "        with open('best_model/model.pkl','wb') as f:\n",
    "            pickle.dump(svm_model,f)\n",
    "            print(\"SVM\")\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_index(row,word_index_dict):  \n",
    "    holder = []\n",
    "    for word in row.split():\n",
    "        if word in word_index_dict:\n",
    "            holder.append(word_index_dict[word]) \n",
    "        else:\n",
    "            holder.append(0)            \n",
    "#     return holder\n",
    "# def lstm_training(x,y):\n",
    "#     total_words = []\n",
    "\n",
    "#     for sent in x:\n",
    "#         words = sent.split()\n",
    "#         total_words+=words\n",
    "#     from collections import Counter\n",
    "#     counter = Counter(total_words)\n",
    "#     top_words_count = int(len(counter)/0.95)\n",
    "#     sorted_words = counter.most_common(top_words_count)\n",
    "\n",
    "#     word_index_dict = dict()\n",
    "#     i = 1\n",
    "#     for word,frequency in sorted_words:\n",
    "#         word_index_dict[word] = i\n",
    "#         i += 1\n",
    "#     pickle.dump(word_index_dict,open('word_index_dict.pkl', 'wb'))\n",
    "#     text=[]\n",
    "#     for t in x:\n",
    "#         text.append(find_word_index(t,word_index_dict))\n",
    "#     label_binarizer = LabelBinarizer()\n",
    "#     labels = label_binarizer.fit_transform(y)\n",
    "#     pickle.dump(label_binarizer,open('label_lstm.pkl', 'wb'))\n",
    "#     n_classes = len(label_binarizer.classes_)\n",
    "#     x_train,x_test,y_train,y_test = train_test_split(text,labels,test_size=0.1,shuffle=True,random_state=35)\n",
    "#     m=0\n",
    "#     for ind in text:\n",
    "#         i=len(ind)\n",
    "#         m=max(m,i)\n",
    "#     max_review_length = m\n",
    "\n",
    "#     x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "#     x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "#     print(\"nclasses:\",n_classes)\n",
    "#     print(\"max length:\",m)\n",
    "#     vocab_size = len(counter.most_common()) + 1\n",
    "#     model = Sequential()\n",
    "\n",
    "# # Add Embedding Layer\n",
    "#     model.add(Embedding(vocab_size, 32, input_length=max_review_length))\n",
    "\n",
    "# # Add batch normalization\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "# # Add dropout\n",
    "#     model.add(Dropout(0.20))\n",
    "\n",
    "# # Add LSTM Layer\n",
    "#     model.add(LSTM(128,return_sequences=True))\n",
    "\n",
    "#     model.add(LSTM(64))\n",
    "\n",
    "# # Add dropout\n",
    "#     model.add(Dropout(0.20))\n",
    "\n",
    "# # Add Dense Layer\n",
    "#     model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# # Summary of the model\n",
    "#     print(\"Model Summary: \\n\")\n",
    "#     print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "#     callbacks = [ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1,restore_best_weights=True)\n",
    "    \n",
    "#     results = model.fit(x_train, np.array(y_train), batch_size = 32, epochs = 1, verbose=2, validation_data=(x_test, y_test),callbacks=[early_stop])\n",
    "#     test_scores = model.evaluate(x_test,y_test,verbose=1)\n",
    "#     accuracy=test_scores[1]\n",
    "#     predicts = model.predict(x_test)\n",
    "    \n",
    "# #     print(\"accuracy::\",accuracy_score(y_test,predicts))\n",
    "#     with open('model_lstm.pkl','wb') as f:\n",
    "#         pickle.dump(model,f)\n",
    "    \n",
    "#     return model,accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_index(row,word_index_dict):  \n",
    "        holder = []\n",
    "        for word in row.split():\n",
    "            if word in word_index_dict:\n",
    "                holder.append(word_index_dict[word]) \n",
    "            else:\n",
    "                holder.append(0)            \n",
    "        return holder\n",
    "\n",
    "\n",
    "\n",
    "def lstm_training(x,y):\n",
    "        total_words = []\n",
    "\n",
    "        for sent in x:\n",
    "            words = sent.split()\n",
    "            total_words+=words\n",
    "        from collections import Counter\n",
    "        counter = Counter(total_words)\n",
    "        top_words_count = int(len(counter)/0.95)\n",
    "        sorted_words = counter.most_common(top_words_count)\n",
    "\n",
    "        word_index_dict = dict()\n",
    "        i = 1\n",
    "        for word,frequency in sorted_words:\n",
    "            word_index_dict[word] = i\n",
    "            i += 1\n",
    "        text=[]\n",
    "        for t in x:\n",
    "            text.append(find_word_index(t,word_index_dict))\n",
    "        label_binarizer = LabelBinarizer()\n",
    "        labels = label_binarizer.fit_transform(y)\n",
    "        n_classes = len(label_binarizer.classes_)\n",
    "        pickle.dump(label_binarizer,open('label_lstm.pkl', 'wb'))\n",
    "        pickle.dump(word_index_dict,open('word_index_dict.pkl', 'wb'))\n",
    "        x_train,x_test,y_train,y_test = train_test_split(text,labels,test_size=0.1,shuffle=True,random_state=35)\n",
    "        m=0\n",
    "        for ind in text:\n",
    "            i=len(ind)\n",
    "            m=max(m,i)\n",
    "        max_review_length = m\n",
    "\n",
    "        x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "        x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "        print(\"nclasses:\",n_classes)\n",
    "        print(\"max length:\",m)\n",
    "        vocab_size = len(counter.most_common()) + 1\n",
    "        model = Sequential()\n",
    "\n",
    "    # Add Embedding Layer\n",
    "        model.add(Embedding(vocab_size, 32, input_length=max_review_length))\n",
    "\n",
    "    # Add batch normalization\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    # Add dropout\n",
    "        model.add(Dropout(0.20))\n",
    "\n",
    "    # Add LSTM Layer\n",
    "        model.add(LSTM(128,return_sequences=True))\n",
    "\n",
    "        model.add(LSTM(64))\n",
    "\n",
    "    # Add dropout\n",
    "        model.add(Dropout(0.20))\n",
    "\n",
    "    # Add Dense Layer\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Summary of the model\n",
    "        print(\"Model Summary: \\n\")\n",
    "        print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "        callbacks = [ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1,restore_best_weights=True)\n",
    "        \n",
    "        results = model.fit(x_train, np.array(y_train), batch_size = 32, epochs = 1, verbose=2, validation_data=(x_test, y_test),callbacks=[early_stop])\n",
    "        test_scores = model.evaluate(x_test,y_test,verbose=1)\n",
    "        accuracy=test_scores[1]\n",
    "        predicts = model.predict(x_test)\n",
    "        \n",
    "    #     print(\"accuracy::\",accuracy_score(y_test,predicts))\n",
    "        with open('model_lstm.pkl','wb') as f:\n",
    "            pickle.dump(model,f)\n",
    "        \n",
    "        return model,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"hlo man how youre doing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "number of classes:: 3\n",
      "done\n",
      "done\n",
      "nclasses: 3\n",
      "max length: 24\n",
      "Model Summary: \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 24, 32)            367552    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 32)            128       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 24, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 24, 128)           82432     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 499,715\n",
      "Trainable params: 499,651\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 13176 samples, validate on 1464 samples\n",
      "Epoch 1/1\n",
      " - 33s - loss: 0.3859 - acc: 0.8282 - val_loss: 0.3414 - val_acc: 0.8472\n",
      "1464/1464 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 960us/step\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    out1,acc_svc=predict_svm(x)\n",
    "    out2,acc_lstm=predict_lstm(x)\n",
    "    if acc_svc>acc_lstm:\n",
    "        print(out1)\n",
    "    else:\n",
    "        print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_svm(x):\n",
    "    x=preprocess(x)\n",
    "    with open('model_svc.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "#     print(model.predict[x])\n",
    "    with open('label_svm.pkl', 'rb') as f:\n",
    "        encoder= pickle.load(f)\n",
    "    \n",
    "  \n",
    "        \n",
    "    x=find_word_index(row,word_index_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pred=model.predict([x])\n",
    "    \n",
    "    pred=encoder.inverse_transform(pred)\n",
    "    print(\"output::\",pred)\n",
    "        \n",
    "    return pred,model.predict_proba([x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_svm(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_index(row,word_index_dict):  \n",
    "    holder = []\n",
    "    for word in row.split():\n",
    "        if word in word_index_dict:\n",
    "            holder.append(word_index_dict[word]) \n",
    "        else:\n",
    "            holder.append(0)            \n",
    "    return holder\n",
    "def predict_lstm(x):\n",
    "    x=preprocess(x)\n",
    "    with open('model_lstm.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "#     print(model.predict[x])\n",
    "    with open('label_lstm.pkl', 'rb') as f:\n",
    "        encoder= pickle.load(f)\n",
    "        \n",
    "    with open('word_index_dict.pkl', 'rb') as f:\n",
    "        word_index_dict= pickle.load(f)\n",
    "        \n",
    "        \n",
    "    x= find_word_index(x,word_index_dict)\n",
    "        \n",
    "    x=sequence.pad_sequences([x], maxlen=24)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pred=model.predict(x)\n",
    "    \n",
    "    print(pred)\n",
    "    print(encoder.classes_)\n",
    "    \n",
    "    pred=encoder.inverse_transform(pred)\n",
    "    print(\"output::\",pred)\n",
    "        \n",
    "    return pred,max(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08871433 0.35324693 0.5580387 ]]\n",
      "['negative' 'neutral' 'positive']\n",
      "output:: ['positive']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype='<U8')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_lstm(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuu",
   "language": "python",
   "name": "gpuu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
