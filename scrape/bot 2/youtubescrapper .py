# -*- coding: utf-8 -*-
"""youtubescrapper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14K86yF3SWE03dxdLlvZNLiKB8weWyjgp
"""

#!pip install google-api-python-client
#!pip install google-auth google-auth-oauthlib google-auth-httplib2

import csv
import os
import re
import nltk
import spacy
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nlp = spacy.load('en')

import pickle
import google.oauth2.credentials
 
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google_auth_oauthlib.flow import InstalledAppFlow
 
# The CLIENT_SECRETS_FILE variable specifies the name of a file that contains
# the OAuth 2.0 information for this application, including its client_id and
# client_secret.
CLIENT_SECRETS_FILE = "client_secret_576215650547-9uvfk2puhiu7915c75okq1t4g7ugs2d7.apps.googleusercontent.com.json"
 
# This OAuth 2.0 access scope allows for full read/write access to the
# authenticated user's account and requires requests to use an SSL connection.
SCOPES = ['https://www.googleapis.com/auth/youtube.force-ssl']
API_SERVICE_NAME = 'youtube'
API_VERSION = 'v3'
 
 
def get_authenticated_service():
    credentials = None
    if os.path.exists('token.pickle'):
        with open('token.pickle', 'rb') as token:
            credentials = pickle.load(token)
    #  Check if the credentials are invalid or do not exist
    if not credentials or not credentials.valid:
        # Check if the credentials have expired
        if credentials and credentials.expired and credentials.refresh_token:
            credentials.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                CLIENT_SECRETS_FILE, SCOPES)
            credentials = flow.run_console()
 
        # Save the credentials for the next run
        with open('token.pickle', 'wb') as token:
            pickle.dump(credentials, token)
 
    return build(API_SERVICE_NAME, API_VERSION, credentials = credentials)
 
 
def get_video_comments(service, **kwargs):
    
    results = service.commentThreads().list(**kwargs).execute()
 
    while results:
        for item in results['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comments.append(comment)
 
        # Check if another page exists
        if 'nextPageToken' in results:
            kwargs['pageToken'] = results['nextPageToken']
            results = service.commentThreads().list(**kwargs).execute()
        else:
            break
 
    return comments
 
 
def write_to_csv(comments):
    with open('comments.csv', 'w') as comments_file:
        comments_writer = csv.writer(comments_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        comments_writer.writerow(['Video ID', 'Title', 'Comment'])
        for row in comments:
            # convert the tuple to a list and write to the output file
            comments_writer.writerow(list(row))
 
 
def get_videos(service, **kwargs):
    
    results = service.search().list(**kwargs).execute()
 
    i = 0
    max_pages = 3
    while results and i < max_pages:
        final_results.extend(results['items'])
 
        # Check if another page exists
        if 'nextPageToken' in results:
            kwargs['pageToken'] = results['nextPageToken']
            results = service.search().list(**kwargs).execute()
            i += 1
        else:
            break
 
    return final_results
 
 
def search_videos_by_keyword(service, **kwargs):
    results = get_videos(service, **kwargs)
    
    for item in results:
        title = item['snippet']['title']
        '''
        print(title)
        rev=re.sub('[^a-zA-Z]',' ',str(title))
        rev=rev.lower();
        doc = nlp(rev, disable=['parser', 'ner'])
        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']
        tokens = [tok for tok in tokens if tok not in nlp.Defaults.stop_words and tok not in punctuations]
        tokens = set(tokens)
        if len(b)==1:
          x=0
        else:
          x=1
        if len(set.intersection(set(tokens), set(b)))>x:
        '''
        video_id = item['id']['videoId']
        comments = get_video_comments(service, part='snippet', videoId=video_id, textFormat='plainText')
        # make a tuple consisting of the video id, title, comment and add the result to 
        # the final list
        final_result.extend([(video_id, title, comment) for comment in comments]) 

    write_to_csv(final_result)
  
if __name__ == '__main__':
    # When running locally, disable OAuthlib's HTTPs verification. When
    # running in production *do not* leave this option enabled.
    os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'
    service = get_authenticated_service()
    comments = []
    final_result = []
    print("enter sentence or list of words:")
    b = input()
    print(b)
    b = b.split(' ')
    b = set(b)
    b = list(b)
    print(b)
    final_results = []
    for i in b:
      try:
        search_videos_by_keyword(service, q=i, part='id,snippet', eventType='completed', type='video')
      except:
        print(i,"NOT SCRAPPED")

import sqlite3
dbname = 'FinanceExplained'
conn = sqlite3.connect(dbname + '.sqlite')
cur = conn.cursor()
import pandas as pd
#if we have a csv file
df = pd.read_csv('comments.csv',sep=',')
df.to_sql(name='newsy', con=conn)
cur.execute('SELECT * FROM newsy')
for i in cur:
  print(i)

